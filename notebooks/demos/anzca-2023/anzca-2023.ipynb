{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a0db354ad54515",
   "metadata": {},
   "source": [
    "# ANZCA 2023 - Digital Methods Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a91983a8ea816b",
   "metadata": {},
   "source": [
    "## Relevant links:\n",
    "\n",
    "Australian Text Analytics Platform (ATAP): https://www.atap.edu.au/\n",
    "\n",
    "Juxtorpus code base: https://github.com/Sydney-Informatics-Hub/juxtorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ed580136e4612",
   "metadata": {},
   "source": [
    "## Juxtorpus: Understanding your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613559fc8373714c",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "**Corpus**\n",
    "\n",
    "A corpus is a collection of documents of a similar kind. An example of a corpus would be essays from a particular author.\n",
    "Documents in a corpus are typically assumed to be in the same format as each other and to share a theme or subject-matter.\n",
    "\n",
    "**Metadata**\n",
    "\n",
    "Metadata is data that describes the dataset in question. Examples of metadata of a news article would be the author or date published. Metadata provides context for data so that we can perform more informed analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1652c485be4f9a",
   "metadata": {},
   "source": [
    "### Example data set: Internet news data with readers engagement\n",
    "Access link: https://www.kaggle.com/datasets/szymonjanowski/internet-articles-data-with-users-engagement\n",
    "\n",
    "Context:\n",
    "\n",
    "This dataset was created and used in order to determine the popularity of an article before it was published online but thanks to its flexibility can be used in various tasks.\n",
    "It contains articles (listed as the top in popularity at the publisher website) from multiple well-known publishers. Then using Facebook GraphAPI data was enriched with engagement features such as shares, reactions, and comments count.\n",
    "\n",
    "Date range: 03-09-2019 to 04-11-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45df14eff573ad7",
   "metadata": {},
   "source": [
    "### Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265944ebd480e31",
   "metadata": {},
   "source": [
    "Data always comes with context and bias. It is part of our job when analysing data to understand the data's context and account for it when drawing conclusions. One method of accounting for the context of data is data cleaning. The following changes to the example data set have been made to more easily achieve our analysis goals:\n",
    "\n",
    "- only the relevant columns were preserved. Columns such as 'url' have been removed. This speeds up processing times and makes loading easier\n",
    "- the article contents column has been edited to improve term frequency analysis:\n",
    "    - most entries in the 'content' column ended with a statement like \"â€¦ \\[+100 chars\\]\". We can infer that this is not from the articles themselves, but is an artifact of the parsing tool used to gather the articles (most likely to keep the file size small). Performing frequency analysis prior to this amendment yielded \"chars\" as by far the most frequently used term\n",
    "- often data analysis tools contain rudamentary data cleaning functionality. In the timeline tool demonstrated in this notebook, there is a checkbox titled \"Remove stopwords\", which allows the ignoring of terms such as \"but\", \"and\", and \"the\". Be mindful of the data cleaning functionalities available to you so that you don't make unnecessary changes to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f586968c4a3ef4c",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47c715648ec647",
   "metadata": {},
   "source": [
    "#### Corpus Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c2859a0c36128",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "while not 'juxtorpus' in os.listdir():\n",
    "    os.chdir('../')\n",
    "from juxtorpus.corpus.corpora import Corpora\n",
    "from juxtorpus import Jux\n",
    "from juxtorpus.viz.item_timeline import ItemTimeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "corpora = Corpora()\n",
    "corpora.widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46771c5bfd6b61e7",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_ls = [corpora.get(c) for c in corpora.items()]\n",
    "corpus_ls[-1].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e17a3b0ab2b02",
   "metadata": {},
   "source": [
    "#### Author publishing frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfda90d9ab93398",
   "metadata": {},
   "source": [
    "In order to run the following cell, ensure you have built a corpus with the document column assigned to the column with the author names (or equivalent).\n",
    "\n",
    "This cell will provide a timeline of publishing frequency for each day by author.\n",
    "\n",
    "**Example data discussion**\n",
    "\n",
    "Below we can start to see some patterns and limitations within our example data:\n",
    "- individual author names are conflated with publication names. We can mitigate this by deselecting the publication names in the visualisation\n",
    "- some publications have duplicate entries (BBC News and https://www.facebook.com/bbcnews)\n",
    "- some dates are conspicuously missing any entries at all (Sept 21st to Sept 25th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d553bc43b14f6",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_ls = [corpora.get(c) for c in corpora.items()]\n",
    "corpus = corpus_ls[-1]\n",
    "corpus.create_custom_dtm(lambda x: [x])\n",
    "timeline = ItemTimeline.from_corpus(corpus, freq='1d', use_custom_dtm=True)\n",
    "timeline.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c45da5bd22274",
   "metadata": {},
   "source": [
    "#### Term frequency over time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c502670e0c4999",
   "metadata": {},
   "source": [
    "In order to run the following cell, ensure you have built a corpus with the document column assigned to the column with the article contents.\n",
    "\n",
    "This cell will provide a timeline of terms used across all articles for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f063e2b014488ce",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_ls = [corpora.get(c) for c in corpora.items()]\n",
    "corpus = corpus_ls[-1]\n",
    "timeline = ItemTimeline.from_corpus(corpus, freq='1d')\n",
    "timeline.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e8984-54ac-45e3-bed3-bf781b7843f3",
   "metadata": {},
   "source": [
    "#### Term frequency by news source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c23afb601ca84",
   "metadata": {},
   "source": [
    "In order to run the following cell, ensure you have built a corpus with the document column assigned to the column with the new source identifier.\n",
    "\n",
    "Additionally, ensure you have sliced the corpus twice by news source.\n",
    "\n",
    "This cell will set up the news source corpora for use in the later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc05c0958f8f51",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_ls = [corpora.get(c) for c in corpora.items()]\n",
    "corpus_a = corpus_ls[-1]\n",
    "corpus_b = corpus_ls[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1139c841f5511e",
   "metadata": {},
   "source": [
    "The following cells will provide wordclouds for each respective corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a2e2129425ba4",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_a.viz.wordcloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a557a9591588bd",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_b.viz.wordcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a00a15444c1f8",
   "metadata": {},
   "source": [
    "**Polarity wordcloud - [Term frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency) mode**\n",
    "\n",
    "The Juxtorpus wordcloud in Term Frequency mode provides a wordcloud with three variables for each term:\n",
    "1. colour: one corpus is assigned red and the other blue.\n",
    "2. opacity: the opacity of a term indicates how much its frequency skews to one corpus, such that if the term is very opaque it occurs much more frequently in one corpus over the other.\n",
    "3. size: the size of a term indicates how distinct that term is to a corpus. The formula for determining the size is (simplified): `size = polarity / total_frequency`. Polarity is the difference between the frequency of the term in one corpus and the other corpus (normalised for corpus size). Dividing by total frequency controls for words that are common across both corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f02b3-9196-45b5-9dd1-33ae894f58f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jux = Jux(corpus_a, corpus_b)\n",
    "jux.polarity.wordcloud('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429704c471599125",
   "metadata": {},
   "source": [
    "**Polarity wordcloud - [Term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency%E2%80%93inverse_document_frequency) mode**\n",
    "\n",
    "An alternative to Term Frequency measurement is Term frequency-inverse document frequency. Tf-idf is the product of Term frequency and Inverse document frequency, which minimises the score for terms that appear in many documents within the corpus. A much more detailed description of tf-idf can be found at the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5aaea9f05c5814",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jux = Jux(corpus_a, corpus_b)\n",
    "jux.polarity.wordcloud('tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26b5ac8ddde51",
   "metadata": {},
   "source": [
    "**Polarity wordcloud - [Log likelihood](https://ucrel.lancs.ac.uk/llwizard.html) mode**\n",
    "\n",
    "Log-likelihood is another alternative measurement that takes into consideration the size of the two corpora when determining the frequency of the term within the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788373b6132ab8cb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jux = Jux(corpus_a, corpus_b)\n",
    "jux.polarity.wordcloud('log_likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other ATAP tools\n",
    "\n",
    "Semantic tagger: tag your text so you can extract token level semantic tags from the tagged text.\n",
    "https://github.com/Australian-Text-Analytics-Platform/semantic-tagger\n",
    "\n",
    "Quotation tool: extract quotes from a text while providing information about the speakers and quote locations.\n",
    "https://github.com/Australian-Text-Analytics-Platform/quotation-tool\n",
    "\n",
    "Discursis: an analysis and visualisation tool for conversational data.\n",
    "https://github.com/Australian-Text-Analytics-Platform/discursis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6565e6ae704aec64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
